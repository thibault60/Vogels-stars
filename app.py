import streamlit as st
import pandas as pd
import json
from serpapi import GoogleSearch
from concurrent.futures import ThreadPoolExecutor, as_completed
from io import BytesIO

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 1. Configuration
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.set_page_config(page_title="VisibilitÃ© Voyage PrivÃ© â€“ SERP", layout="wide")

try:
    SERPAPI_KEY = st.secrets["serpapi_key"]
except Exception:
    st.error("âŒ ClÃ© SerpApi manquante dans `.streamlit/secrets.toml`.")
    st.stop()

VP_DOMAIN = "voyage-prive.com"

DEFAULT_QUERIES = """voyage en ThaÃ¯lande
sÃ©jour tout compris pas cher
hÃ´tel bord de mer"""

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 2. Sidebar
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
with st.sidebar:
    st.header("âš™ï¸ ParamÃ¨tres")
    hl = st.selectbox("Langue (hl)", ["fr", "en", "es", "de", "it"], index=0)
    gl = st.selectbox("Pays (gl)", ["fr", "us", "es", "de", "it"], index=0)
    num_results = st.slider("RÃ©sultats organiques analysÃ©s", 10, 30, 10, step=10)
    max_workers = st.slider("Threads simultanÃ©s", 1, 8, 3)

    st.markdown("---")
    debug_mode = st.toggle("ğŸ› Mode debug", value=False, help="Affiche la structure brute SerpApi pour identifier les clÃ©s du carrousel")

    st.markdown("---")
    st.markdown(
        "**Sources extraites**\n\n"
        "- ğŸ”µ Lien bleu principal (rÃ©sultat organique VP)\n"
        "- ğŸ  Sitelinks carrousel (offres VP sous le rÃ©sultat)"
    )

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 3. Zone de saisie des requÃªtes
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
st.title("ğŸ” VisibilitÃ© Voyage PrivÃ© â€“ Liens organiques & Sitelinks")
st.markdown(
    "Extrait les **URLs `voyage-prive.com`** depuis :\n"
    "- ğŸ”µ Le **lien bleu principal** dans les rÃ©sultats organiques\n"
    "- ğŸ  Les **sitelinks en carrousel** (offres affichÃ©es sous le rÃ©sultat principal)"
)

queries_raw = st.text_area(
    "ğŸ“‹ Liste de requÃªtes (une par ligne)",
    value=DEFAULT_QUERIES,
    height=160,
)
queries = [q.strip() for q in queries_raw.splitlines() if q.strip()]
st.caption(f"**{len(queries)} requÃªte(s)** chargÃ©e(s)")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 4. Fonctions d'extraction
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

def fetch_raw(query: str, hl: str, gl: str, num: int) -> dict:
    """Retourne la rÃ©ponse brute SerpApi."""
    params = {
        "q": query,
        "api_key": SERPAPI_KEY,
        "hl": hl,
        "gl": gl,
        "num": num,
        "engine": "google",
    }
    search = GoogleSearch(params)
    return search.get_dict()


def extract_vp_results(query: str, hl: str, gl: str, num: int) -> tuple[list[dict], dict]:
    """
    Retourne (rows, raw_data).
    rows = liste de dicts pour le tableau rÃ©sultats
    raw_data = rÃ©ponse brute SerpApi (pour le mode debug)
    """
    try:
        data = fetch_raw(query, hl, gl, num)
    except Exception as exc:
        return [_row(query, "âš ï¸ Erreur API", "â€”", str(exc), "", "â€”")], {}

    rows = []
    organic = data.get("organic_results", [])

    for pos, result in enumerate(organic, start=1):
        main_link = result.get("link", "")

        if VP_DOMAIN in main_link:
            rows.append(_row(
                query   = query,
                type_   = "ğŸ”µ Lien principal",
                position= pos,
                titre   = result.get("title", "â€”"),
                url     = main_link,
                snippet = result.get("snippet", "â€”"),
            ))

            # Tentative sur toutes les structures connues de sitelinks
            sitelinks_data = result.get("sitelinks", {})

            if isinstance(sitelinks_data, dict):
                inline_links = (
                    sitelinks_data.get("inline", [])
                    or sitelinks_data.get("expanded", [])
                    or sitelinks_data.get("list", [])
                )
            elif isinstance(sitelinks_data, list):
                inline_links = sitelinks_data
            else:
                inline_links = []

            for idx, sl in enumerate(inline_links, start=1):
                sl_link = sl.get("link", "") or sl.get("url", "")
                if VP_DOMAIN in sl_link:
                    rows.append(_row(
                        query   = query,
                        type_   = "ğŸ  Sitelink carrousel",
                        position= f"{pos}.{idx}",
                        titre   = sl.get("title", "â€”"),
                        url     = sl_link,
                        snippet = sl.get("snippet", ""),
                    ))

    if not rows:
        rows.append(_row(query, "âŒ Absent", "â€”", "Voyage PrivÃ© absent des rÃ©sultats", "", ""))

    return rows, data


def _row(query, type_, position, titre, url, snippet) -> dict:
    return {
        "RequÃªte" : query,
        "Type"    : type_,
        "Position": position,
        "Titre"   : titre,
        "URL"     : url,
        "Snippet" : snippet,
    }


@st.cache_data(ttl=3_600, show_spinner=False)
def run_all(queries_tuple: tuple, hl: str, gl: str, num: int, workers: int) -> tuple[pd.DataFrame, dict]:
    all_rows = []
    all_raw  = {}  # {query: raw_data}
    progress = st.progress(0.0, text="ğŸ”„ Analyse des SERPâ€¦")
    with ThreadPoolExecutor(max_workers=workers) as executor:
        futures = {
            executor.submit(extract_vp_results, q, hl, gl, num): q
            for q in queries_tuple
        }
        total = len(futures)
        for i, future in enumerate(as_completed(futures), 1):
            rows, raw = future.result()
            all_rows.extend(rows)
            q = futures[future]
            all_raw[q] = raw
            progress.progress(i / total, text=f"ğŸ”„ {i}/{total} requÃªtes analysÃ©esâ€¦")
    progress.empty()
    return pd.DataFrame(all_rows), all_raw


# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
# 5. Lancement + Affichage
# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

if st.button("ğŸš€ Lancer l'extraction", type="primary", disabled=len(queries) == 0):

    df, raw_data = run_all(tuple(queries), hl, gl, num_results, max_workers)

    # â”€â”€ KPIs â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    principal = df[df["Type"] == "ğŸ”µ Lien principal"]
    sitelinks = df[df["Type"] == "ğŸ  Sitelink carrousel"]
    absent    = df[df["Type"] == "âŒ Absent"]

    c1, c2, c3, c4 = st.columns(4)
    c1.metric("RequÃªtes analysÃ©es",     len(queries))
    c2.metric("ğŸ”µ Liens principaux VP", len(principal))
    c3.metric("ğŸ  Sitelinks offres VP", len(sitelinks))
    c4.metric("âŒ Sans prÃ©sence VP",    len(absent))

    st.markdown("---")

    # â”€â”€ Tableau principal â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.subheader("ğŸ“Š RÃ©sultats dÃ©taillÃ©s")

    type_filter = st.multiselect(
        "Filtrer par type",
        options=df["Type"].unique().tolist(),
        default=df["Type"].unique().tolist(),
    )
    df_filtered = df[df["Type"].isin(type_filter)]

    st.dataframe(
        df_filtered,
        use_container_width=True,
        height=450,
        column_config={
            "URL": st.column_config.LinkColumn("URL", display_text="ğŸ”— Voir"),
        },
        column_order=["RequÃªte", "Type", "Position", "Titre", "URL", "Snippet"],
    )

    # â”€â”€ Vue groupÃ©e par requÃªte â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    st.subheader("ğŸ” DÃ©tail par requÃªte")

    for query in df["RequÃªte"].unique():
        subset   = df[df["RequÃªte"] == query]
        nb_sl    = len(subset[subset["Type"] == "ğŸ  Sitelink carrousel"])
        has_main = len(subset[subset["Type"] == "ğŸ”µ Lien principal"]) > 0
        has_vp   = has_main or nb_sl > 0

        badge_main = "ğŸ”µ lien principal" if has_main else ""
        badge_sl   = f"+ ğŸ  {nb_sl} sitelink(s)" if nb_sl else ""
        label = f"{'âœ…' if has_vp else 'âŒ'} {query} â€” {badge_main} {badge_sl}".strip(" â€”")

        with st.expander(label):
            if not has_vp:
                st.info("Voyage PrivÃ© n'apparaÃ®t pas dans les rÃ©sultats analysÃ©s.")
            else:
                main_rows = subset[subset["Type"] == "ğŸ”µ Lien principal"]
                if not main_rows.empty:
                    r = main_rows.iloc[0]
                    st.markdown(f"**ğŸ”µ RÃ©sultat principal â€” Position `{r['Position']}`**")
                    st.markdown(f"**{r['Titre']}**")
                    st.markdown(f"[{r['URL']}]({r['URL']})")
                    if r["Snippet"] and r["Snippet"] not in ("â€”", ""):
                        st.caption(r["Snippet"])
                    st.markdown("---")

                sl_rows = subset[subset["Type"] == "ğŸ  Sitelink carrousel"]
                if not sl_rows.empty:
                    st.markdown(f"**ğŸ  Sitelinks carrousel â€” {len(sl_rows)} offre(s)**")
                    for _, r in sl_rows.iterrows():
                        col_a, col_b = st.columns([2, 8])
                        col_a.markdown(f"Pos. `{r['Position']}`")
                        col_b.markdown(f"**{r['Titre']}** â†’ [{r['URL']}]({r['URL']})")

            # â”€â”€ ğŸ› MODE DEBUG â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
            if debug_mode and query in raw_data:
                st.markdown("---")
                st.markdown("**ğŸ› Structure brute SerpApi â€” rÃ©sultats VP uniquement**")

                vp_results = [
                    r for r in raw_data[query].get("organic_results", [])
                    if VP_DOMAIN in r.get("link", "")
                ]

                if vp_results:
                    for r in vp_results:
                        st.markdown(f"**Position {r.get('position')} â€” ClÃ©s disponibles :**")
                        # Affiche toutes les clÃ©s avec leur type et valeur tronquÃ©e
                        keys_info = {
                            k: f"{type(v).__name__} â†’ {str(v)[:150]}"
                            for k, v in r.items()
                        }
                        st.json(keys_info)

                        # Focus sur sitelinks si prÃ©sents
                        if "sitelinks" in r:
                            st.markdown("**ğŸ¯ ClÃ© `sitelinks` (structure complÃ¨te) :**")
                            st.json(r["sitelinks"])
                        else:
                            st.warning("âš ï¸ Pas de clÃ© `sitelinks` dans ce rÃ©sultat â€” le carrousel n'est pas parsÃ© par SerpApi pour cette requÃªte.")
                else:
                    st.info("Aucun rÃ©sultat VP dans la rÃ©ponse brute.")

    # â”€â”€ Exports â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    st.markdown("---")
    col1, col2 = st.columns(2)

    csv = df.to_csv(index=False).encode("utf-8")
    col1.download_button(
        "ğŸ’¾ TÃ©lÃ©charger CSV",
        data=csv,
        file_name="vp_serp_sitelinks.csv",
        mime="text/csv",
    )

    xlsx_buffer = BytesIO()
    with pd.ExcelWriter(xlsx_buffer, engine="xlsxwriter") as writer:
        df.to_excel(writer, index=False, sheet_name="DÃ©tail")
        summary = (
            df[~df["Type"].isin(["âŒ Absent", "âš ï¸ Erreur API"])]
            .groupby(["RequÃªte", "Type"])
            .size()
            .reset_index(name="Nombre URLs VP")
        )
        summary.to_excel(writer, index=False, sheet_name="RÃ©sumÃ©")
    xlsx_buffer.seek(0)
    col2.download_button(
        "ğŸ“Š TÃ©lÃ©charger XLSX",
        data=xlsx_buffer,
        file_name="vp_serp_sitelinks.xlsx",
        mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
    )
